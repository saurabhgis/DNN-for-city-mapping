@article{Kita-wojciechowska,
author = {Kita-wojciechowska, Kinga and Sciences, Economic},
file = {:D$\backslash$:/Projects/DNN for city mapping/Documentation/Individual project report/Research Papers/1904.05270.pdf:pdf},
title = {{Google Street View image of a house predicts car accident risk of its resident}}
}
@article{Law2018,
abstract = {When an individual purchases a home, they simultaneously purchase its structural features, its accessibility to work, and the neighborhood amenities. Some amenities, such as air quality, are measurable whilst others, such as the prestige or the visual impression of a neighborhood, are difficult to quantify. Despite the well-known impacts intangible housing features have on house prices, limited attention has been given to systematically quantifying these difficult to measure amenities. Two issues have lead to this neglect. Not only do few quantitative methods exist that can measure the urban environment, but that the collection of such data is both costly and subjective. We show that street image and satellite image data can capture these urban qualities and improve the estimation of house prices. We propose a pipeline that uses a deep neural network model to automatically extract visual features from images to estimate house prices in London, UK. We make use of traditional housing features such as age, size and accessibility as well as visual features from Google Street View images and Bing aerial images in estimating the house price model. We find encouraging results where learning to characterize the urban quality of a neighborhood improves house price prediction, even when generalizing to previously unseen London boroughs. We explore the use of non-linear vs. linear methods to fuse these cues with conventional models of house pricing, and show how the interpretability of linear models allows us to directly extract the visual desirability of neighborhoods as proxy variables that are both of interest in their own right, and could be used as inputs to other econometric methods. This is particularly valuable as once the network has been trained with the training data, it can be applied elsewhere, allowing us to generate vivid dense maps of the desirability of London streets.},
archivePrefix = {arXiv},
arxivId = {1807.07155},
author = {Law, Stephen and Paige, Brooks and Russell, Chris},
eprint = {1807.07155},
file = {:D$\backslash$:/Projects/DNN for city mapping/Documentation/Individual project report/Research Papers/1807.07155.pdf:pdf},
isbn = {1234567245},
keywords = {convolutional neural network,deep learning,hedonic,london,machine vision,or,or hard copies of,part or all of,permission to make digital,price models,real estate,this work for personal},
title = {{Take a Look Around: Using Street View and Satellite Images to Estimate House Prices}},
url = {http://arxiv.org/abs/1807.07155},
year = {2018}
}
@article{Kang2018a,
abstract = {Land-use classification based on spaceborne or aerial remote sensing images has been extensively studied over the past decades. Such classification is usually a patch-wise or pixel-wise labeling over the whole image. But for many applications, such as urban population density mapping or urban utility planning, a classification map based on individual buildings is much more informative. However, such semantic classification still poses some fundamental challenges, for example, how to retrieve fine boundaries of individual buildings. In this paper, we proposed a general framework for classifying the functionality of individual buildings. The proposed method is based on Convolutional Neural Networks (CNNs) which classify fa{\c{c}}ade structures from street view images, such as Google StreetView, in addition to remote sensing images which usually only show roof structures. Geographic information was utilized to mask out individual buildings, and to associate the corresponding street view images. We created a benchmark dataset which was used for training and evaluating CNNs. In addition, the method was applied to generate building classification maps on both region and city scales of several cities in Canada and the US.},
author = {Kang, Jian and K{\"{o}}rner, Marco and Wang, Yuanyuan and Taubenb{\"{o}}ck, Hannes and Zhu, Xiao Xiang},
doi = {10.1016/j.isprsjprs.2018.02.006},
file = {:D$\backslash$:/Projects/DNN for city mapping/Documentation/Individual project report/Research Papers/1-s2.0-S0924271618300352-main.pdf:pdf},
issn = {09242716},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
keywords = {Building instance classification,CNN,OpenStreetMap,Street view images},
pages = {44--59},
publisher = {The Author(s)},
title = {{Building instance classification using street view images}},
url = {https://doi.org/10.1016/j.isprsjprs.2018.02.006},
volume = {145},
year = {2018}
}
@misc{Batista,
author = {Batista, David S.},
booktitle = {2018-02-23},
title = {{Hyperparameter optimization across multiple models in scikit-learn}},
url = {http://www.davidsbatista.net/blog/2018/02/23/model{\_}optimization/}
}
@misc{Posik2019,
abstract = {Neural networks. Basic models and methods, error backpropagation},
author = {Posik, Petr},
title = {{Neural networks. Basic models and methods, error backpropagation}},
url = {https://cw.fel.cvut.cz/wiki/{\_}media/courses/ui/b12nn-slides.pdf},
year = {2019}
}
@article{Tahmasebi2017,
abstract = {With advances in technology and culture, our language changes. We invent new words, add or change meanings of existing words and change names of existing things. Unfortunately, our language does not carry a memory; words, expressions and meanings used in the past are forgotten over time. When searching and interpreting content from archives, language changes pose a great challenge. In this paper, we present results of automatic word sense change detection and show the utility for archive users as well as digital humanities' research. Our method is able to capture changes that relate to the usage and culture of a word that cannot easily be found using dictionaries or other resources.},
author = {Tahmasebi, Nina and Risse, Thomas},
doi = {10.1007/978-3-319-67008-9_20},
file = {:C$\backslash$:/Users/varun/Downloads/ch1.pdf:pdf},
isbn = {9783319670072},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {246--257},
title = {{On the uses of word sense change for research in the digital humanities}},
volume = {10450 LNCS},
year = {2017}
}
@inproceedings{sklearn_api,
author = {Buitinck, Lars and Louppe, Gilles and Blondel, Mathieu and Pedregosa, Fabian and Mueller, Andreas and Grisel, Olivier and Niculae, Vlad and Prettenhofer, Peter and Gramfort, Alexandre and Grobler, Jaques and Layton, Robert and VanderPlas, Jake and Joly, Arnaud and Holt, Brian and Varoquaux, Ga{\"{e}}l},
booktitle = {ECML PKDD Workshop: Languages for Data Mining and Machine Learning},
pages = {108--122},
title = {{{\{}API{\}} design for machine learning software: experiences from the scikit-learn project}},
year = {2013}
}
@misc{Story2019,
abstract = {Documentation 0.8.3},
author = {Story, Rob},
title = {{Folium}},
url = {https://python-visualization.github.io/folium/},
year = {2019}
}
@misc{En.wikipedia.org2019,
author = {En.wikipedia.org},
title = {{Bilinear interpolation}},
url = {https://en.wikipedia.org/wiki/Bilinear{\_}interpolation},
year = {2019}
}
@article{Girshick2015,
abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.},
archivePrefix = {arXiv},
arxivId = {1504.08083},
author = {Girshick, Ross},
doi = {10.1109/ICCV.2015.169},
eprint = {1504.08083},
file = {:D$\backslash$:/Projects/Python/Individual project/report/Research Papers/1504.08083.pdf:pdf},
isbn = {9781467383912},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {1440--1448},
pmid = {23739795},
title = {{Fast R-CNN}},
volume = {2015 Inter},
year = {2015}
}
@article{Lin2017,
abstract = {Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But recent deep learning object detectors have avoided pyramid representations, in part because they are compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using FPN in a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.},
archivePrefix = {arXiv},
arxivId = {1612.03144},
author = {Lin, Tsung Yi and Doll{\'{a}}r, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
doi = {10.1109/CVPR.2017.106},
eprint = {1612.03144},
file = {:D$\backslash$:/Projects/DNN for city mapping/Documentation/Individual project report/Research Papers/1612.03144.pdf:pdf},
isbn = {9781538604571},
issn = {0006-291X},
journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
pages = {936--944},
pmid = {303902},
title = {{Feature pyramid networks for object detection}},
volume = {2017-Janua},
year = {2017}
}
@misc{Public2018,
author = {Public},
title = {pycocoapi},
url = {https://github.com/cocodataset/cocoapi/blob/master/PythonAPI/pycocotools/coco.py},
year = {2018}
}
@article{Kang2018,
abstract = {Land-use classification based on spaceborne or aerial remote sensing images has been extensively studied over the past decades. Such classification is usually a patch-wise or pixel-wise labeling over the whole image. But for many applications, such as urban population density mapping or urban utility planning, a classification map based on individual buildings is much more informative. However, such semantic classification still poses some fundamental challenges, for example, how to retrieve fine boundaries of individual buildings. In this paper, we proposed a general framework for classifying the functionality of individual buildings. The proposed method is based on Convolutional Neural Networks (CNNs) which classify fa{\c{c}}ade structures from street view images, such as Google StreetView, in addition to remote sensing images which usually only show roof structures. Geographic information was utilized to mask out individual buildings, and to associate the corresponding street view images. We created a benchmark dataset which was used for training and evaluating CNNs. In addition, the method was applied to generate building classification maps on both region and city scales of several cities in Canada and the US.},
archivePrefix = {arXiv},
arxivId = {1802.09026},
author = {Kang, Jian and K{\"{o}}rner, Marco and Wang, Yuanyuan and Taubenb{\"{o}}ck, Hannes and Zhu, Xiao Xiang},
doi = {10.1016/j.isprsjprs.2018.02.006},
eprint = {1802.09026},
file = {:C$\backslash$:/Users/varun/Downloads/1-s2.0-S0924271618300352-main.pdf:pdf},
issn = {09242716},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
keywords = {Building instance classification,CNN,OpenStreetMap,Street view images},
pages = {44--59},
publisher = {The Author(s)},
title = {{Building instance classification using street view images}},
url = {https://doi.org/10.1016/j.isprsjprs.2018.02.006},
volume = {145},
year = {2018}
}
@misc{JohnHunterDarrenDaleEricFiring2012,
author = {{John Hunter, Darren Dale, Eric Firing}, Michael Droettboom},
title = {{Matplotlib}},
url = {https://matplotlib.org/},
year = {2012}
}
@misc{Jung2017,
author = {Jung, Alexander},
title = {{imguag Documentation}},
url = {https://imgaug.readthedocs.io/en/latest/source/installation.html},
year = {2017}
}
@article{Andriluka2014,
abstract = {Human pose estimation has made significant progress during the last years. However current datasets are limited in their coverage of the overall pose estimation challenges. Still these serve as the common sources to evaluate, train and compare different models on. In this paper we introduce a novel benchmark "MPII Human Pose" that makes a significant advance in terms of diversity and difficulty, a contribution that we feel is required for future developments in human body models. This comprehensive dataset was collected using an established taxonomy of over 800 human activities [1]. The collected images cover a wider variety of human activities than previous datasets including various recreational, occupational and householding activities, and capture people from a wider range of viewpoints. We provide a rich set of labels including positions of body joints, full 3D torso and head orientation, occlusion labels for joints and body parts, and activity labels. For each image we provide adjacent video frames to facilitate the use of motion information. Given these rich annotations we perform a detailed analysis of leading human pose estimation approaches and gaining insights for the success and failures of these methods.},
archivePrefix = {arXiv},
arxivId = {1405.0312v3},
author = {Andriluka, Mykhaylo and Pishchulin, Leonid and Gehler, Peter and Schiele, Bernt},
doi = {10.1109/CVPR.2014.471},
eprint = {1405.0312v3},
file = {:D$\backslash$:/Projects/DNN for city mapping/Documentation/Individual project report/Research Papers/1405.0312.pdf:pdf},
isbn = {9781479951178},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
keywords = {human pose estimation,performance evaluation},
pages = {3686--3693},
title = {{2D human pose estimation: New benchmark and state of the art analysis}},
year = {2014}
}
@article{DBLP:journals/corr/RenHG015,
archivePrefix = {arXiv},
arxivId = {1506.01497},
author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross B and Sun, Jian},
eprint = {1506.01497},
journal = {CoRR},
title = {{Faster {\{}R-CNN:{\}} Towards Real-Time Object Detection with Region Proposal Networks}},
url = {http://arxiv.org/abs/1506.01497},
volume = {abs/1506.0},
year = {2015}
}
@misc{Pytorch.org2018,
author = {Pytorch.org},
title = {{PYTORCH DOCUMENTATION}},
url = {https://pytorch.org/docs/stable/index.html},
year = {2018}
}
@misc{Rosebrock2017,
author = {Rosebrock, Adrian},
title = {{ImageNet: VGGNet, ResNet, Inception, and Xception with Keras}},
url = {https://www.pyimagesearch.com/2017/03/20/imagenet-vggnet-resnet-inception-xception-keras/},
year = {2017}
}
@misc{Jupyter.org2019,
author = {Jupyter.org},
title = {{Documentation}},
url = {https://jupyter.org/documentation},
year = {2019}
}
@article{DBLP:journals/corr/LinMBHPRDZ14,
archivePrefix = {arXiv},
arxivId = {1405.0312},
author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge J and Bourdev, Lubomir D and Girshick, Ross B and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'{a}}r, Piotr and Zitnick, C Lawrence},
eprint = {1405.0312},
journal = {CoRR},
title = {{Microsoft {\{}COCO:{\}} Common Objects in Context}},
url = {http://arxiv.org/abs/1405.0312},
volume = {abs/1405.0},
year = {2014}
}
@book{Stockman:2001:CV:558008,
address = {Upper Saddle River, NJ, USA},
author = {Stockman, George and Shapiro, Linda G},
edition = {1st},
isbn = {0130307963},
publisher = {Prentice Hall PTR},
title = {{Computer Vision}},
year = {2001}
}
@misc{Http://cs231n.stanford.edu/,
author = {Http://cs231n.stanford.edu/},
title = {{Image Classification}},
url = {http://cs231n.github.io/classification/}
}
@misc{,
file = {:C$\backslash$:/Users/varun/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - 558008.txt:txt},
title = {558008}
}
@misc{En.wikipedia.org,
author = {En.wikipedia.org},
booktitle = {2018},
title = {{Contextual Image classification}},
url = {https://en.wikipedia.org/wiki/Contextual{\_}image{\_}classification}
}
@misc{,
title = {{TensorFLow}},
url = {https://www.tensorflow.org/guide}
}
@article{Michaelis2018,
abstract = {We tackle one-shot visual search by example for arbitrary object categories: Given an example image of a novel reference object, find and segment all object instances of the same category within a scene. To address this problem, we propose Siamese Mask R-CNN. It extends Mask R-CNN by a Siamese backbone encoding both reference image and scene, allowing it to target detection and segmentation towards the reference category. We use Siamese Mask R-CNN to perform one-shot instance segmentation on MS-COCO, demonstrating that it can detect and segment objects of novel categories it was not trained on, and without using mask annotations at test time. Our results highlight challenges of the one-shot setting: while transferring knowledge about instance segmentation to novel object categories not used during training works very well, targeting the detection and segmentation networks towards the reference category appears to be more difficult. Our work provides a first strong baseline for one-shot instance segmentation and will hopefully inspire further research in this relatively unexplored field.},
archivePrefix = {arXiv},
arxivId = {1811.11507},
author = {Michaelis, Claudio and Ustyuzhaninov, Ivan and Bethge, Matthias and Ecker, Alexander S.},
eprint = {1811.11507},
file = {:C$\backslash$:/Users/varun/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Michaelis et al. - 2018 - One-Shot Instance Segmentation.pdf:pdf},
isbn = {1811.11507v1},
title = {{One-Shot Instance Segmentation}},
url = {http://arxiv.org/abs/1811.11507},
year = {2018}
}
@misc{PythonSoftwareFoundation2019,
author = {{Python Software Foundation}},
title = {{Python}},
url = {https://www.python.org/about/},
year = {2019}
}
@misc{Colab.research.google.com,
author = {Colab.research.google.com},
title = {{Welcome to Colaboratory!}},
url = {https://colab.research.google.com/notebooks/welcome.ipynb}
}
@misc{Conservancy,
author = {Conservancy, Software Freedom},
title = {git --fast-version-control},
url = {https://git-scm.com/site}
}
@misc{,
title = {{Keras Documentation}},
url = {https://keras.io/},
year = {2019}
}
@article{Howard2017,
abstract = {We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.},
archivePrefix = {arXiv},
arxivId = {1704.04861},
author = {Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
doi = {10.1016/S1507-1367(10)60022-3},
eprint = {1704.04861},
file = {:C$\backslash$:/Users/varun/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Howard et al. - 2017 - MobileNets Efficient Convolutional Neural Networks for Mobile Vision Applications.pdf:pdf},
isbn = {2004012439},
issn = {15071367},
pmid = {23459267},
title = {{MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications}},
url = {http://arxiv.org/abs/1704.04861},
year = {2017}
}
@misc{Valigi2016,
author = {Valigi, Nicol{\`{o}}},
title = {{A review of deep learning models for semantic segmentation}},
url = {https://nicolovaligi.com/deep-learning-models-semantic-segmentation.html},
year = {2016}
}
@article{Culjak2012,
abstract = {The purpose of this paper is to introduce and quickly make a reader familiar with OpenCV (Open Source Computer Vision) basics without having to go through the lengthy reference manuals and books. OpenCV is an open source library for image and video analysis, originally introduced more than decade ago by Intel. Since then, a number of programmers have contributed to the most recent library developments. The latest major change took place in 2009 (OpenCV 2) which includes main changes to the C++ interface. Nowadays the library has {\textgreater};2500 optimized algorithms. It is extensively used around the world, having {\textgreater};2.5M downloads and {\textgreater};40K people in the user group. Regardless of whether one is a novice C++ programmer or a professional software developer, unaware of OpenCV, the main library content should be interesting for the graduate students and researchers in image processing and computer vision areas. To master every library element it is necessary to consult many books available on the topic of OpenCV. However, reading such more comprehensive material should be easier after comprehending some basics about OpenCV from this paper.},
author = {Culjak, Ivan and Abram, David and Pribanic, Tomislav and Dzapo, Hrvoje and Cifrek, Mario},
doi = {978-1-4673-2577-6},
file = {:C$\backslash$:/Users/varun/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Culjak et al. - 2012 - A brief introduction to OpenCV.pdf:pdf},
isbn = {VO -},
issn = {14712148},
journal = {MIPRO, 2012 Proceedings of the 35th International Convention},
pages = {1725--1730},
publisher = {IEEE},
title = {{A brief introduction to OpenCV}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6240859},
year = {2012}
}
@article{Russakovsky2015,
abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the five years of the challenge, and propose future directions and improvements.},
archivePrefix = {arXiv},
arxivId = {1409.0575},
author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
doi = {10.1007/s11263-015-0816-y},
eprint = {1409.0575},
file = {:C$\backslash$:/Users/varun/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Russakovsky et al. - 2015 - ImageNet Large Scale Visual Recognition Challenge.pdf:pdf},
isbn = {0920-5691},
issn = {15731405},
journal = {International Journal of Computer Vision},
keywords = {Benchmark,Dataset,Large-scale,Object detection,Object recognition},
number = {3},
pages = {211--252},
pmid = {16190471},
title = {{ImageNet Large Scale Visual Recognition Challenge}},
volume = {115},
year = {2015}
}
@misc{StanfordVisionLab2015,
abstract = {The ImageNet Large Scale Visual Recognition Challenge (ILSVRC) evaluates algorithms for object detection and image classification at large scale. One high level motivation is to allow researchers to compare progress in detection across a wider variety of objects -- taking advantage of the quite expensive labeling effort. Another motivation is to measure the progress of computer vision for large scale image indexing for retrieval and annotation.},
author = {{Stanford Vision Lab}},
title = {{Large Scale Visual Recognition Challenge (ILSVRC)}},
url = {http://www.image-net.org/challenges/LSVRC/},
year = {2015}
}
@misc{Developers.google.com2018,
abstract = {Google Street view API Documentation},
author = {Developers.google.com},
booktitle = {December 19, 2018.},
title = {{Developer Guide}},
url = {https://developers.google.com/maps/documentation/streetview/intro},
year = {2018}
}
@article{Szegedy2015,
abstract = {Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2{\%} top-1 and 5.6{\%} top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5{\%} top-5 error on the validation set (3.6{\%} error on the test set) and 17.3{\%} top-1 error on the validation set.},
archivePrefix = {arXiv},
arxivId = {1512.00567},
author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jonathon and Wojna, Zbigniew},
doi = {10.1109/CVPR.2016.308},
eprint = {1512.00567},
file = {:C$\backslash$:/Users/varun/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Szegedy et al. - 2015 - Rethinking the Inception Architecture for Computer Vision.pdf:pdf},
isbn = {978-1-4673-8851-1},
issn = {08866236},
pmid = {8190083},
title = {{Rethinking the Inception Architecture for Computer Vision}},
url = {http://arxiv.org/abs/1512.00567},
year = {2015}
}
@article{Zoph,
archivePrefix = {arXiv},
arxivId = {arXiv:1707.07012v4},
author = {Zoph, Barret and Shlens, Jonathon},
eprint = {arXiv:1707.07012v4},
file = {:C$\backslash$:/Users/varun/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zoph, Shlens - Unknown - Learning Transferable Architectures for Scalable Image Recognition.pdf:pdf},
title = {{Learning Transferable Architectures for Scalable Image Recognition}}
}
@article{Huang2017,
abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet .},
archivePrefix = {arXiv},
arxivId = {1608.06993},
author = {Huang, Gao and Liu, Zhuang and {Van Der Maaten}, Laurens and Weinberger, Kilian Q.},
doi = {10.1109/CVPR.2017.243},
eprint = {1608.06993},
file = {:C$\backslash$:/Users/varun/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Huang et al. - 2017 - Densely connected convolutional networks.pdf:pdf},
isbn = {9781538604571},
issn = {0022-4790},
journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
pages = {2261--2269},
pmid = {211888},
title = {{Densely connected convolutional networks}},
volume = {2017-Janua},
year = {2017}
}
@article{Simonyan2014,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
archivePrefix = {arXiv},
arxivId = {1409.1556},
author = {Simonyan, Karen and Zisserman, Andrew},
doi = {10.2146/ajhp170251},
eprint = {1409.1556},
file = {:C$\backslash$:/Users/varun/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Simonyan, Zisserman - 2014 - Very Deep Convolutional Networks for Large-Scale Image Recognition.pdf:pdf},
isbn = {1097-0142 (Electronic)$\backslash$n0008-543X (Linking)},
issn = {15352900},
journal = {American Journal of Health-System Pharmacy},
keywords = {CPOE,Chemotherapy,Medication error,Safety-net hospital,Usability,Workflow},
month = {sep},
number = {6},
pages = {398--406},
pmid = {8940136},
title = {{Very Deep Convolutional Networks for Large-Scale Image Recognition}},
url = {http://arxiv.org/abs/1409.1556},
volume = {75},
year = {2014}
}
@article{Carneiro2018,
abstract = {Google Colaboratory (also known as Colab) is a cloud service based on Jupyter Notebooks for disseminating machine learning education and research. It provides a runtime fully configured for deep learning and free-of-charge access to a robust GPU. This paper presents a detailed analysis of Colaboratory regarding hardware resources, performance, and limitations. This analysis is performed through the use of Colaboratory for accelerating deep learning for computer vision and other GPU-centric applications. The chosen test-cases are a parallel tree-based combinatorial search and two computer vision applications: object detection/classification and object localization/segmentation. The hardware under the accelerated runtime is compared with a mainstream workstation and a robust Linux server equipped with 20 physical cores. Results show that the performance reached using this cloud service is equivalent to the performance of the dedicated testbeds, given similar resources. Thus, this service can be effectively exploited to accelerate not only deep learning but also other classes of GPU-centric applications. For instance, it is faster to train a CNN on Colaboratory's accelerated runtime than using 20 physical cores of a Linux server. The performance of the GPU made available by Colaboratory may be enough for several profiles of researchers and students. However, these free-of-charge hardware resources are far from enough to solve demanding real-world problems and are not scalable. The most significant limitation found is the lack of CPU cores. Finally, several strengths and limitations of this cloud service are discussed, which might be useful for helping potential users.},
author = {Carneiro, Tiago and {Da Nobrega}, Raul Victor Medeiros and Nepomuceno, Thiago and Bian, Gui Bin and {De Albuquerque}, Victor Hugo C. and Filho, Pedro Pedrosa Reboucas},
doi = {10.1109/ACCESS.2018.2874767},
file = {:D$\backslash$:/Projects/DNN for city mapping/Documentation/Individual project report/Research Papers/08485684.pdf:pdf},
issn = {21693536},
journal = {IEEE Access},
keywords = {Colab,Deep learning,GPU computing,Google colaboratory,convolutional neural networks},
pages = {61677--61685},
title = {{Performance Analysis of Google Colaboratory as a Tool for Accelerating Deep Learning Applications}},
volume = {6},
year = {2018}
}
@book{2017,
author = {林伸行},
booktitle = {感染症誌},
doi = {0130307963},
file = {:C$\backslash$:/Users/varun/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/林伸行 - 2017 - 病院・介護施設におけるノロウイルス感染症の拡大防止対策を 目的とした吐物の飛散状況に関する研究No Title.pdf:pdf},
pages = {399--404},
title = {{病院・介護施設におけるノロウイルス感染症の拡大防止対策を 目的とした吐物の飛散状況に関する研究No Title}},
volume = {91},
year = {2017}
}
@article{Liu2015,
abstract = {We propose a layered street view model to encode both depth and semantic information on street view images for autonomous driving. Recently, stixels, stix-mantics, and tiered scene labeling methods have been proposed to model street view images. We propose a 4-layer street view model, a compact representation over the recently proposed stix-mantics model. Our layers encode semantic classes like ground, pedestrians, vehicles, buildings, and sky in addition to the depths. The only input to our algorithm is a pair of stereo images. We use a deep neural network to extract the appearance features for semantic classes. We use a simple and an efficient inference algorithm to jointly estimate both semantic classes and layered depth values. Our method outperforms other competing approaches in Daimler urban scene segmentation dataset. Our algorithm is massively parallelizable, allowing a GPU implementation with a processing speed about 9 fps.},
archivePrefix = {arXiv},
arxivId = {1506.04723},
author = {Liu, Ming-Yu and Lin, Shuoxin and Ramalingam, Srikumar and Tuzel, Oncel},
doi = {10.15607/RSS.2015.XI.025},
eprint = {1506.04723},
file = {:C$\backslash$:/Users/varun/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2015 - Layered Interpretation of Street View Images.pdf:pdf},
isbn = {9780992374716},
issn = {2330765X},
title = {{Layered Interpretation of Street View Images}},
url = {http://arxiv.org/abs/1506.04723},
year = {2015}
}
@article{He2017,
abstract = {We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron},
archivePrefix = {arXiv},
arxivId = {1703.06870},
author = {He, Kaiming and Gkioxari, Georgia and Dollar, Piotr and Girshick, Ross},
doi = {10.1109/ICCV.2017.322},
eprint = {1703.06870},
file = {:C$\backslash$:/Users/varun/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/He et al. - 2017 - Mask R-CNN.pdf:pdf},
isbn = {9781538610329},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {2980--2988},
pmid = {1706716},
title = {{Mask R-CNN}},
volume = {2017-Octob},
year = {2017}
}
@article{He2015,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
doi = {10.1016/0141-0229(95)00188-3},
eprint = {1512.03385},
file = {:C$\backslash$:/Users/varun/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:pdf},
isbn = {0141-0229},
issn = {01410229},
journal = {Enzyme and Microbial Technology},
keywords = {Fructooligosaccharides,Fructosyltransferase,Oligofructosides,Sweetener},
month = {dec},
number = {2},
pages = {107--117},
pmid = {22931231},
title = {{Deep Residual Learning for Image Recognition}},
url = {http://arxiv.org/abs/1512.03385},
volume = {19},
year = {2015}
}
@article{Szegedy2016,
abstract = {Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08 percent top-5 error on the test set of the ImageNet classification (CLS) challenge},
archivePrefix = {arXiv},
arxivId = {1602.07261},
author = {Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alex},
doi = {10.1016/S0022-5347(17)50340-7},
eprint = {1602.07261},
file = {:C$\backslash$:/Users/varun/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Szegedy et al. - 2016 - Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning.pdf:pdf},
isbn = {0022-5347 (Print)$\backslash$r0022-5347 (Linking)},
issn = {00225347},
journal = {Journal of Urology},
month = {feb},
number = {2},
pages = {262--263},
pmid = {6699954},
title = {{Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning}},
url = {http://arxiv.org/abs/1602.07261},
volume = {131},
year = {2016}
}
@article{Sandler2018,
abstract = {In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3. The MobileNetV2 architecture is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input an MobileNetV2 uses lightweight depthwise convolutions to filter features in the intermediate expansion layer. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on Imagenet classification, COCO object detection, VOC image segmentation. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as the number of parameters},
archivePrefix = {arXiv},
arxivId = {1801.04381},
author = {Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
doi = {10.1134/S0001434607010294},
eprint = {1801.04381},
file = {:C$\backslash$:/Users/varun/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sandler et al. - 2018 - MobileNetV2 Inverted Residuals and Linear Bottlenecks.pdf:pdf},
isbn = {9781538610329},
issn = {08695652},
pmid = {23766329},
title = {{MobileNetV2: Inverted Residuals and Linear Bottlenecks}},
url = {http://arxiv.org/abs/1801.04381},
year = {2018}
}
@article{Chollet2017,
abstract = {We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.},
archivePrefix = {arXiv},
arxivId = {1610.02357},
author = {Chollet, Fran{\c{c}}ois},
doi = {10.1109/CVPR.2017.195},
eprint = {1610.02357},
file = {:C$\backslash$:/Users/varun/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chollet - 2017 - Xception Deep learning with depthwise separable convolutions.pdf:pdf},
isbn = {9781538604571},
issn = {ISSN: 0305-8719 CODEN: GSSPDQ ISSN: 0263-080X DOI: 10.1108/02630800210433828},
journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
pages = {1800--1807},
pmid = {4881142},
title = {{Xception: Deep learning with depthwise separable convolutions}},
volume = {2017-Janua},
year = {2017}
}
@article{Chamber1982,
abstract = {JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org.},
archivePrefix = {arXiv},
arxivId = {1603.04467v2},
author = {Chamber, M. A. and Montes, F. J.},
doi = {10.1007/BF02183801},
eprint = {1603.04467v2},
file = {:C$\backslash$:/Users/varun/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chamber, Montes - 1982 - Effects of some seeds disinfectants and methods of rhizobial inoculation on soybeans (Glycine max L. Merrill).pdf:pdf},
isbn = {9780123849885},
issn = {0032079X},
journal = {Plant and Soil},
keywords = {Fungicides,Inoculation,N2-Fixation,Nodulation,Rhizobium japonicum,Seeds disinfection,Soybeans},
number = {3},
pages = {353--360},
title = {{Effects of some seeds disinfectants and methods of rhizobial inoculation on soybeans (Glycine max L. Merrill)}},
volume = {66},
year = {1982}
}
@article{Kim2018,
abstract = {Many recent object detection algorithms use the bounding box regressor to predict the position coordinates of an object (i.e., to predict four continuous variables of an object's bounding box information). To improve object detection accuracy, we propose four types of object boundary segmentation masks that provide position information in a different manner than that done by object detection algorithms, Additionally, we investigated the effect of the proposed object bounding shape masks on instance segmentation. To evaluate the proposed masks, our method adds a proposed bounding shape (or box) mask to extend the Faster R-CNN framework; we call this Bounding Shape (or Box) Mask R-CNN. We experimentally verified its performance with two benchmark datasets, MS COCO and Cityscapes. The results indicate that our proposed models generally outperform Faster R-CNN and Mask R-CNN.},
archivePrefix = {arXiv},
arxivId = {1810.10327},
author = {Kim, Ha Young and Kang, Ba Rom},
doi = {arXiv:1810.10327v1},
eprint = {1810.10327},
file = {:C$\backslash$:/Users/varun/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kim, Kang - 2018 - Instance Segmentation and Object Detection with Bounding Shape Masks.pdf:pdf},
title = {{Instance Segmentation and Object Detection with Bounding Shape Masks}},
url = {http://arxiv.org/abs/1810.10327},
year = {2018}
}
@article{Murakami1988,
archivePrefix = {arXiv},
arxivId = {arXiv:1506.01497v3},
author = {Murakami, T. and Uede, T. and Ide, T. and Kikuchi, K.},
eprint = {arXiv:1506.01497v3},
file = {:C$\backslash$:/Users/varun/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Murakami et al. - 1988 - Interleukin receptors and mechanism of target cell activation.pdf:pdf},
issn = {00471852},
journal = {Nippon rinsho. Japanese journal of clinical medicine},
number = {5},
pages = {988--993},
title = {{Interleukin receptors and mechanism of target cell activation}},
volume = {46},
year = {1988}
}

@misc{matterport_maskrcnn_2017,
  title={Mask R-CNN for object detection and instance segmentation on Keras and TensorFlow},
  author={Waleed Abdulla},
  year={2017},
  publisher={Github},
  journal={GitHub repository},
  howpublished={\url{https://github.com/matterport/Mask_RCNN}},
}

@article{Carneiro2018,
abstract = {Google Colaboratory (also known as Colab) is a cloud service based on Jupyter Notebooks for disseminating machine learning education and research. It provides a runtime fully configured for deep learning and free-of-charge access to a robust GPU. This paper presents a detailed analysis of Colaboratory regarding hardware resources, performance, and limitations. This analysis is performed through the use of Colaboratory for accelerating deep learning for computer vision and other GPU-centric applications. The chosen test-cases are a parallel tree-based combinatorial search and two computer vision applications: object detection/classification and object localization/segmentation. The hardware under the accelerated runtime is compared with a mainstream workstation and a robust Linux server equipped with 20 physical cores. Results show that the performance reached using this cloud service is equivalent to the performance of the dedicated testbeds, given similar resources. Thus, this service can be effectively exploited to accelerate not only deep learning but also other classes of GPU-centric applications. For instance, it is faster to train a CNN on Colaboratory's accelerated runtime than using 20 physical cores of a Linux server. The performance of the GPU made available by Colaboratory may be enough for several profiles of researchers and students. However, these free-of-charge hardware resources are far from enough to solve demanding real-world problems and are not scalable. The most significant limitation found is the lack of CPU cores. Finally, several strengths and limitations of this cloud service are discussed, which might be useful for helping potential users.},
author = {Carneiro, Tiago and {Da Nobrega}, Raul Victor Medeiros and Nepomuceno, Thiago and Bian, Gui Bin and {De Albuquerque}, Victor Hugo C. and Filho, Pedro Pedrosa Reboucas},
doi = {10.1109/ACCESS.2018.2874767},
file = {:D$\backslash$:/Projects/DNN for city mapping/Documentation/Individual project report/Research Papers/08485684.pdf:pdf},
issn = {21693536},
journal = {IEEE Access},
keywords = {Colab,Deep learning,GPU computing,Google colaboratory,convolutional neural networks},
pages = {61677--61685},
title = {{Performance Analysis of Google Colaboratory as a Tool for Accelerating Deep Learning Applications}},
volume = {6},
year = {2018}
}

