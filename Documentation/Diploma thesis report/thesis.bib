@misc{,
title = {{TensorFLow}},
url = {https://www.tensorflow.org/guide}
}
@misc{,
title = {{Keras Documentation}},
url = {https://keras.io/},
year = {2019}
}
@article{Chamber1982,
abstract = {JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org.},
archivePrefix = {arXiv},
arxivId = {1603.04467v2},
author = {Chamber, M. A. and Montes, F. J.},
doi = {10.1007/BF02183801},
eprint = {1603.04467v2},
file = {:C$\backslash$:/Users/varun/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chamber, Montes - 1982 - Effects of some seeds disinfectants and methods of rhizobial inoculation on soybeans (Glycine max L. Merrill).pdf:pdf},
isbn = {9780123849885},
issn = {0032079X},
journal = {Plant and Soil},
keywords = {Fungicides,Inoculation,N2-Fixation,Nodulation,Rhizobium japonicum,Seeds disinfection,Soybeans},
number = {3},
pages = {353--360},
title = {{Effects of some seeds disinfectants and methods of rhizobial inoculation on soybeans (Glycine max L. Merrill)}},
volume = {66},
year = {1982}
}
@article{Chollet2017,
abstract = {We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.},
archivePrefix = {arXiv},
arxivId = {1610.02357},
author = {Chollet, Fran{\c{c}}ois},
doi = {10.1109/CVPR.2017.195},
eprint = {1610.02357},
file = {:C$\backslash$:/Users/varun/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chollet - 2017 - Xception Deep learning with depthwise separable convolutions.pdf:pdf},
isbn = {9781538604571},
issn = {ISSN: 0305-8719 CODEN: GSSPDQ ISSN: 0263-080X DOI: 10.1108/02630800210433828},
journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
pages = {1800--1807},
pmid = {4881142},
title = {{Xception: Deep learning with depthwise separable convolutions}},
volume = {2017-Janua},
year = {2017}
}
@misc{Colab.research.google.com,
author = {Colab.research.google.com},
title = {{Welcome to Colaboratory!}},
url = {https://colab.research.google.com/notebooks/welcome.ipynb}
}
@misc{Conservancy,
author = {Conservancy, Software Freedom},
title = {git --fast-version-control},
url = {https://git-scm.com/site}
}
@article{Culjak2012,
abstract = {The purpose of this paper is to introduce and quickly make a reader familiar with OpenCV (Open Source Computer Vision) basics without having to go through the lengthy reference manuals and books. OpenCV is an open source library for image and video analysis, originally introduced more than decade ago by Intel. Since then, a number of programmers have contributed to the most recent library developments. The latest major change took place in 2009 (OpenCV 2) which includes main changes to the C++ interface. Nowadays the library has {\textgreater};2500 optimized algorithms. It is extensively used around the world, having {\textgreater};2.5M downloads and {\textgreater};40K people in the user group. Regardless of whether one is a novice C++ programmer or a professional software developer, unaware of OpenCV, the main library content should be interesting for the graduate students and researchers in image processing and computer vision areas. To master every library element it is necessary to consult many books available on the topic of OpenCV. However, reading such more comprehensive material should be easier after comprehending some basics about OpenCV from this paper.},
author = {Culjak, Ivan and Abram, David and Pribanic, Tomislav and Dzapo, Hrvoje and Cifrek, Mario},
doi = {978-1-4673-2577-6},
file = {:C$\backslash$:/Users/varun/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Culjak et al. - 2012 - A brief introduction to OpenCV.pdf:pdf},
isbn = {VO -},
issn = {14712148},
journal = {MIPRO, 2012 Proceedings of the 35th International Convention},
pages = {1725--1730},
publisher = {IEEE},
title = {{A brief introduction to OpenCV}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6240859},
year = {2012}
}
@misc{Developers.google.com2018,
abstract = {Google Street view API Documentation},
author = {Developers.google.com},
booktitle = {December 19, 2018.},
title = {{Developer Guide}},
url = {https://developers.google.com/maps/documentation/streetview/intro},
year = {2018}
}
@misc{En.wikipedia.org2019,
author = {En.wikipedia.org},
title = {{Bilinear interpolation}},
url = {https://en.wikipedia.org/wiki/Bilinear{\_}interpolation},
year = {2019}
}
@misc{En.wikipedia.org,
author = {En.wikipedia.org},
booktitle = {2018},
title = {{Contextual Image classification}},
url = {https://en.wikipedia.org/wiki/Contextual{\_}image{\_}classification}
}
@article{Girshick2015,
abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.},
archivePrefix = {arXiv},
arxivId = {1504.08083},
author = {Girshick, Ross},
doi = {10.1109/ICCV.2015.169},
eprint = {1504.08083},
file = {:D$\backslash$:/Projects/Python/Individual project/report/Research Papers/1504.08083.pdf:pdf},
isbn = {9781467383912},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {1440--1448},
pmid = {23739795},
title = {{Fast R-CNN}},
volume = {2015 Inter},
year = {2015}
}
@article{He2017,
abstract = {We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron},
archivePrefix = {arXiv},
arxivId = {1703.06870},
author = {He, Kaiming and Gkioxari, Georgia and Dollar, Piotr and Girshick, Ross},
doi = {10.1109/ICCV.2017.322},
eprint = {1703.06870},
file = {:C$\backslash$:/Users/varun/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/He et al. - 2017 - Mask R-CNN.pdf:pdf},
isbn = {9781538610329},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {2980--2988},
pmid = {1706716},
title = {{Mask R-CNN}},
volume = {2017-Octob},
year = {2017}
}
@article{He2015,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
doi = {10.1016/0141-0229(95)00188-3},
eprint = {1512.03385},
file = {:C$\backslash$:/Users/varun/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:pdf},
isbn = {0141-0229},
issn = {01410229},
journal = {Enzyme and Microbial Technology},
keywords = {Fructooligosaccharides,Fructosyltransferase,Oligofructosides,Sweetener},
month = {dec},
number = {2},
pages = {107--117},
pmid = {22931231},
title = {{Deep Residual Learning for Image Recognition}},
url = {http://arxiv.org/abs/1512.03385},
volume = {19},
year = {2015}
}
@article{Howard2017,
abstract = {We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.},
archivePrefix = {arXiv},
arxivId = {1704.04861},
author = {Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
doi = {10.1016/S1507-1367(10)60022-3},
eprint = {1704.04861},
file = {:C$\backslash$:/Users/varun/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Howard et al. - 2017 - MobileNets Efficient Convolutional Neural Networks for Mobile Vision Applications.pdf:pdf},
isbn = {2004012439},
issn = {15071367},
pmid = {23459267},
title = {{MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications}},
url = {http://arxiv.org/abs/1704.04861},
year = {2017}
}
@misc{Http://cs231n.stanford.edu/,
author = {Http://cs231n.stanford.edu/},
title = {{Image Classification}},
url = {http://cs231n.github.io/classification/}
}
@article{Huang2017,
abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet .},
archivePrefix = {arXiv},
arxivId = {1608.06993},
author = {Huang, Gao and Liu, Zhuang and {Van Der Maaten}, Laurens and Weinberger, Kilian Q.},
doi = {10.1109/CVPR.2017.243},
eprint = {1608.06993},
file = {:C$\backslash$:/Users/varun/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Huang et al. - 2017 - Densely connected convolutional networks.pdf:pdf},
isbn = {9781538604571},
issn = {0022-4790},
journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
pages = {2261--2269},
pmid = {211888},
title = {{Densely connected convolutional networks}},
volume = {2017-Janua},
year = {2017}
}
@misc{JohnHunterDarrenDaleEricFiring2012,
author = {{John Hunter, Darren Dale, Eric Firing}, Michael Droettboom},
title = {{Matplotlib}},
url = {https://matplotlib.org/},
year = {2012}
}
@misc{Jung2017,
author = {Jung, Alexander},
title = {{imguag Documentation}},
url = {https://imgaug.readthedocs.io/en/latest/source/installation.html},
year = {2017}
}
@misc{Jupyter.org2019,
author = {Jupyter.org},
title = {{Documentation}},
url = {https://jupyter.org/documentation},
year = {2019}
}
@article{Kang2018,
abstract = {Land-use classification based on spaceborne or aerial remote sensing images has been extensively studied over the past decades. Such classification is usually a patch-wise or pixel-wise labeling over the whole image. But for many applications, such as urban population density mapping or urban utility planning, a classification map based on individual buildings is much more informative. However, such semantic classification still poses some fundamental challenges, for example, how to retrieve fine boundaries of individual buildings. In this paper, we proposed a general framework for classifying the functionality of individual buildings. The proposed method is based on Convolutional Neural Networks (CNNs) which classify fa{\c{c}}ade structures from street view images, such as Google StreetView, in addition to remote sensing images which usually only show roof structures. Geographic information was utilized to mask out individual buildings, and to associate the corresponding street view images. We created a benchmark dataset which was used for training and evaluating CNNs. In addition, the method was applied to generate building classification maps on both region and city scales of several cities in Canada and the US.},
archivePrefix = {arXiv},
arxivId = {1802.09026},
author = {Kang, Jian and K{\"{o}}rner, Marco and Wang, Yuanyuan and Taubenb{\"{o}}ck, Hannes and Zhu, Xiao Xiang},
doi = {10.1016/j.isprsjprs.2018.02.006},
eprint = {1802.09026},
file = {:C$\backslash$:/Users/varun/Downloads/1-s2.0-S0924271618300352-main.pdf:pdf},
issn = {09242716},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
keywords = {Building instance classification,CNN,OpenStreetMap,Street view images},
pages = {44--59},
publisher = {The Author(s)},
title = {{Building instance classification using street view images}},
url = {https://doi.org/10.1016/j.isprsjprs.2018.02.006},
volume = {145},
year = {2018}
}
@article{Kim2018,
abstract = {Many recent object detection algorithms use the bounding box regressor to predict the position coordinates of an object (i.e., to predict four continuous variables of an object's bounding box information). To improve object detection accuracy, we propose four types of object boundary segmentation masks that provide position information in a different manner than that done by object detection algorithms, Additionally, we investigated the effect of the proposed object bounding shape masks on instance segmentation. To evaluate the proposed masks, our method adds a proposed bounding shape (or box) mask to extend the Faster R-CNN framework; we call this Bounding Shape (or Box) Mask R-CNN. We experimentally verified its performance with two benchmark datasets, MS COCO and Cityscapes. The results indicate that our proposed models generally outperform Faster R-CNN and Mask R-CNN.},
archivePrefix = {arXiv},
arxivId = {1810.10327},
author = {Kim, Ha Young and Kang, Ba Rom},
doi = {arXiv:1810.10327v1},
eprint = {1810.10327},
file = {:C$\backslash$:/Users/varun/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kim, Kang - 2018 - Instance Segmentation and Object Detection with Bounding Shape Masks.pdf:pdf},
title = {{Instance Segmentation and Object Detection with Bounding Shape Masks}},
url = {http://arxiv.org/abs/1810.10327},
year = {2018}
}
@article{Lin2017,
abstract = {Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But recent deep learning object detectors have avoided pyramid representations, in part because they are compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using FPN in a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.},
archivePrefix = {arXiv},
arxivId = {1612.03144},
author = {Lin, Tsung Yi and Doll{\'{a}}r, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
doi = {10.1109/CVPR.2017.106},
eprint = {1612.03144},
file = {:D$\backslash$:/Projects/Python/Individual project/report/Research Papers/1612.03144.pdf:pdf},
isbn = {9781538604571},
issn = {0006-291X},
journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
pages = {936--944},
pmid = {303902},
title = {{Feature pyramid networks for object detection}},
volume = {2017-Janua},
year = {2017}
}
@article{DBLP:journals/corr/LinMBHPRDZ14,
archivePrefix = {arXiv},
arxivId = {1405.0312},
author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge J and Bourdev, Lubomir D and Girshick, Ross B and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'{a}}r, Piotr and Zitnick, C Lawrence},
eprint = {1405.0312},
journal = {CoRR},
title = {{Microsoft {\{}COCO:{\}} Common Objects in Context}},
url = {http://arxiv.org/abs/1405.0312},
volume = {abs/1405.0},
year = {2014}
}
@article{Liu2015,
abstract = {We propose a layered street view model to encode both depth and semantic information on street view images for autonomous driving. Recently, stixels, stix-mantics, and tiered scene labeling methods have been proposed to model street view images. We propose a 4-layer street view model, a compact representation over the recently proposed stix-mantics model. Our layers encode semantic classes like ground, pedestrians, vehicles, buildings, and sky in addition to the depths. The only input to our algorithm is a pair of stereo images. We use a deep neural network to extract the appearance features for semantic classes. We use a simple and an efficient inference algorithm to jointly estimate both semantic classes and layered depth values. Our method outperforms other competing approaches in Daimler urban scene segmentation dataset. Our algorithm is massively parallelizable, allowing a GPU implementation with a processing speed about 9 fps.},
archivePrefix = {arXiv},
arxivId = {1506.04723},
author = {Liu, Ming-Yu and Lin, Shuoxin and Ramalingam, Srikumar and Tuzel, Oncel},
doi = {10.15607/RSS.2015.XI.025},
eprint = {1506.04723},
file = {:C$\backslash$:/Users/varun/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2015 - Layered Interpretation of Street View Images.pdf:pdf},
isbn = {9780992374716},
issn = {2330765X},
title = {{Layered Interpretation of Street View Images}},
url = {http://arxiv.org/abs/1506.04723},
year = {2015}
}
@article{Michaelis2018,
abstract = {We tackle one-shot visual search by example for arbitrary object categories: Given an example image of a novel reference object, find and segment all object instances of the same category within a scene. To address this problem, we propose Siamese Mask R-CNN. It extends Mask R-CNN by a Siamese backbone encoding both reference image and scene, allowing it to target detection and segmentation towards the reference category. We use Siamese Mask R-CNN to perform one-shot instance segmentation on MS-COCO, demonstrating that it can detect and segment objects of novel categories it was not trained on, and without using mask annotations at test time. Our results highlight challenges of the one-shot setting: while transferring knowledge about instance segmentation to novel object categories not used during training works very well, targeting the detection and segmentation networks towards the reference category appears to be more difficult. Our work provides a first strong baseline for one-shot instance segmentation and will hopefully inspire further research in this relatively unexplored field.},
archivePrefix = {arXiv},
arxivId = {1811.11507},
author = {Michaelis, Claudio and Ustyuzhaninov, Ivan and Bethge, Matthias and Ecker, Alexander S.},
eprint = {1811.11507},
file = {:C$\backslash$:/Users/varun/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Michaelis et al. - 2018 - One-Shot Instance Segmentation.pdf:pdf},
isbn = {1811.11507v1},
title = {{One-Shot Instance Segmentation}},
url = {http://arxiv.org/abs/1811.11507},
year = {2018}
}
@misc{Public2018,
author = {Public},
title = {pycocoapi},
url = {https://github.com/cocodataset/cocoapi/blob/master/PythonAPI/pycocotools/coco.py},
year = {2018}
}
@misc{PythonSoftwareFoundation2019,
author = {{Python Software Foundation}},
title = {{Python}},
url = {https://www.python.org/about/},
year = {2019}
}
@misc{Pytorch.org2018,
author = {Pytorch.org},
title = {{PYTORCH DOCUMENTATION}},
url = {https://pytorch.org/docs/stable/index.html},
year = {2018}
}
@article{DBLP:journals/corr/RenHG015,
archivePrefix = {arXiv},
arxivId = {1506.01497},
author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross B and Sun, Jian},
eprint = {1506.01497},
journal = {CoRR},
title = {{Faster {\{}R-CNN:{\}} Towards Real-Time Object Detection with Region Proposal Networks}},
url = {http://arxiv.org/abs/1506.01497},
volume = {abs/1506.0},
year = {2015}
}
@misc{Rosebrock2017,
author = {Rosebrock, Adrian},
title = {{ImageNet: VGGNet, ResNet, Inception, and Xception with Keras}},
url = {https://www.pyimagesearch.com/2017/03/20/imagenet-vggnet-resnet-inception-xception-keras/},
year = {2017}
}
@article{Russakovsky2015,
abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the five years of the challenge, and propose future directions and improvements.},
archivePrefix = {arXiv},
arxivId = {1409.0575},
author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
doi = {10.1007/s11263-015-0816-y},
eprint = {1409.0575},
file = {:C$\backslash$:/Users/varun/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Russakovsky et al. - 2015 - ImageNet Large Scale Visual Recognition Challenge.pdf:pdf},
isbn = {0920-5691},
issn = {15731405},
journal = {International Journal of Computer Vision},
keywords = {Benchmark,Dataset,Large-scale,Object detection,Object recognition},
number = {3},
pages = {211--252},
pmid = {16190471},
title = {{ImageNet Large Scale Visual Recognition Challenge}},
volume = {115},
year = {2015}
}
@article{Sandler2018,
abstract = {In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3. The MobileNetV2 architecture is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input an MobileNetV2 uses lightweight depthwise convolutions to filter features in the intermediate expansion layer. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on Imagenet classification, COCO object detection, VOC image segmentation. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as the number of parameters},
archivePrefix = {arXiv},
arxivId = {1801.04381},
author = {Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
doi = {10.1134/S0001434607010294},
eprint = {1801.04381},
file = {:C$\backslash$:/Users/varun/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sandler et al. - 2018 - MobileNetV2 Inverted Residuals and Linear Bottlenecks.pdf:pdf},
isbn = {9781538610329},
issn = {08695652},
pmid = {23766329},
title = {{MobileNetV2: Inverted Residuals and Linear Bottlenecks}},
url = {http://arxiv.org/abs/1801.04381},
year = {2018}
}
@article{Simonyan2014,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
archivePrefix = {arXiv},
arxivId = {1409.1556},
author = {Simonyan, Karen and Zisserman, Andrew},
doi = {10.2146/ajhp170251},
eprint = {1409.1556},
file = {:C$\backslash$:/Users/varun/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Simonyan, Zisserman - 2014 - Very Deep Convolutional Networks for Large-Scale Image Recognition.pdf:pdf},
isbn = {1097-0142 (Electronic)$\backslash$n0008-543X (Linking)},
issn = {15352900},
journal = {American Journal of Health-System Pharmacy},
keywords = {CPOE,Chemotherapy,Medication error,Safety-net hospital,Usability,Workflow},
month = {sep},
number = {6},
pages = {398--406},
pmid = {8940136},
title = {{Very Deep Convolutional Networks for Large-Scale Image Recognition}},
url = {http://arxiv.org/abs/1409.1556},
volume = {75},
year = {2014}
}
@misc{StanfordVisionLab2015,
abstract = {The ImageNet Large Scale Visual Recognition Challenge (ILSVRC) evaluates algorithms for object detection and image classification at large scale. One high level motivation is to allow researchers to compare progress in detection across a wider variety of objects -- taking advantage of the quite expensive labeling effort. Another motivation is to measure the progress of computer vision for large scale image indexing for retrieval and annotation.},
author = {{Stanford Vision Lab}},
title = {{Large Scale Visual Recognition Challenge (ILSVRC)}},
url = {http://www.image-net.org/challenges/LSVRC/},
year = {2015}
}
@book{Stockman:2001:CV:558008,
address = {Upper Saddle River, NJ, USA},
author = {Stockman, George and Shapiro, Linda G},
edition = {1st},
isbn = {0130307963},
publisher = {Prentice Hall PTR},
title = {{Computer Vision}},
year = {2001}
}
@article{Szegedy2016,
abstract = {Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08 percent top-5 error on the test set of the ImageNet classification (CLS) challenge},
archivePrefix = {arXiv},
arxivId = {1602.07261},
author = {Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alex},
doi = {10.1016/S0022-5347(17)50340-7},
eprint = {1602.07261},
file = {:C$\backslash$:/Users/varun/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Szegedy et al. - 2016 - Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning.pdf:pdf},
isbn = {0022-5347 (Print)$\backslash$r0022-5347 (Linking)},
issn = {00225347},
journal = {Journal of Urology},
month = {feb},
number = {2},
pages = {262--263},
pmid = {6699954},
title = {{Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning}},
url = {http://arxiv.org/abs/1602.07261},
volume = {131},
year = {2016}
}
@article{Szegedy2015,
abstract = {Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2{\%} top-1 and 5.6{\%} top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5{\%} top-5 error on the validation set (3.6{\%} error on the test set) and 17.3{\%} top-1 error on the validation set.},
archivePrefix = {arXiv},
arxivId = {1512.00567},
author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jonathon and Wojna, Zbigniew},
doi = {10.1109/CVPR.2016.308},
eprint = {1512.00567},
file = {:C$\backslash$:/Users/varun/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Szegedy et al. - 2015 - Rethinking the Inception Architecture for Computer Vision.pdf:pdf},
isbn = {978-1-4673-8851-1},
issn = {08866236},
pmid = {8190083},
title = {{Rethinking the Inception Architecture for Computer Vision}},
url = {http://arxiv.org/abs/1512.00567},
year = {2015}
}
@misc{Valigi2016,
author = {Valigi, Nicol{\`{o}}},
title = {{A review of deep learning models for semantic segmentation}},
url = {https://nicolovaligi.com/deep-learning-models-semantic-segmentation.html},
year = {2016}
}
@article{Zoph,
archivePrefix = {arXiv},
arxivId = {arXiv:1707.07012v4},
author = {Zoph, Barret and Shlens, Jonathon},
eprint = {arXiv:1707.07012v4},
file = {:C$\backslash$:/Users/varun/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zoph, Shlens - Unknown - Learning Transferable Architectures for Scalable Image Recognition.pdf:pdf},
title = {{Learning Transferable Architectures for Scalable Image Recognition}}
}
@misc{matterport_maskrcnn_2017,
title={Mask R-CNN for object detection and instance segmentation on Keras and TensorFlow},
author={Waleed Abdulla},
  year={2017},
  publisher={Github},
  journal={GitHub repository},
  howpublished={\url{https://github.com/matterport/Mask_RCNN}},
}